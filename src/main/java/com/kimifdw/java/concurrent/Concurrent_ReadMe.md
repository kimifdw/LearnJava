# 并发
## 术语定义
1. 共享变量：在多个线程之间能够被共享的变量。他们被存放在堆内存中，Volatile只作用于共享变量
2. 内存屏障(Memory Barriers)：是一组处理器指令，用于实现对内存操作的顺序限制；
3. 缓冲行(Cache line)：缓存中可以分配的最小存储单位。
4. 原子操作（Atomic operation）: 不可中断的一个或一系列操作；
5. 缓存行填充（cache line fill）：当处理器识别到从内存中读取操作数是可缓存的，处理器读取整个缓存行到适当的缓存；
6. 缓存命中（cache hit）：如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数；
7. 写命中(write hit): 当处理器将操作数写回到一个内存缓存的区域时，它首先会检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则将处理器这个操作数写回到内存；
8. 写缺失（write misses the cache)：一个有效的缓存行被写入到不存在的内存区域；
9. CAS(Compare and Swap): CAS操作需要输入两个值，一个旧值和一个新值，在操作期间先比较下旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化则不交换。
10. 哈希算法：是一种将任意内容的输入转换成相同长度输出的加密方式；
11. 哈希表：根据设定的哈希函数和处理冲突方法，将一组关键字映射到一个有限的地址区间上，并以关键字所在地址区间中的象作为记录在表中的存储位置；
12. Ncpu：cpu的数量【Runtime.getRuntime().availableProcessors()】
13. Ucpu：cpu当前的利用率
14. W/C: 等待时间和计算时间的占比
15. Nthreads=Ncpu*Ucpu*(1+W/C)
16. CPU流水线(CPU pipeline)：CPU流水线的工作方式就象工业生产上的装配流水线，在CPU中由5~6个不同功能的电路单元组成一条指令处理流水线，然后将一条X86指令分成5~6步后再由这些电路单元分别执行，这样就能实现在一个CPU时钟周期完成一条指令，因此提高CPU的运算速度。
17. 内存顺序冲突：内存顺序冲突一般是由假共享引起，假共享是指多个CPU同时修改同一个缓存行的不同部分而引起其中一个CPU的操作无效，当出现这个内存顺序冲突时，CPU必须清空流水线。
18. 总线锁：使用处理器提供的一个LOCK#信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。
19. 缓存锁：如果缓存在处理器缓存行中内存区域在LOCK操作期间被锁定，当它执行锁操作回写内存时，处理器不在总线上声言LOCK # 信号，而是修改内部的内存地址，并允许缓存一致性机制来保证操作的原子性。
20. 公平访问队列：阻塞的所有生产者线程或消费者线程，当队列可用时，可以按照阻塞的先后顺序访问队列，即先阻塞的生产者线程，可以先往队列里插入元素，先阻塞的消费者线程，可以先从队列里获取元素。
21. 工作窃取算法：指某个线程从其他队列里窃取任务来执行。优点是充分利用线程进行并行计算，并减少了线程间的竞争，缺点还是存在竞争
22. 生产者消费者模式：通过一个容器来解决生产者和消费者的强耦合问题。用来解决的是消费者和生产者速率不一致而产生的阻抗不匹配。
23. 回调地狱（callback hell）现象。在追踪代码在回调过程中到底做了什么，以及确保每个回调只访问它需要的数据的时候，变得非常困难。
24. 竞态条件：当两个线程竞争同一资源时，如果对资源的访问顺序敏感，就称为存在竞态条件。
25. 临界区：导致竞态条件发生的代码区。
26. 互斥：这个资源只有我用，你不许用；原子：我一口气把活干完，不间断。
27. 线程安全的代码：允许被多个线程同时执行的代码
## 并发模型
1. 并行工作者。委派者(Delegator)将传入的作业分配给不同的工作者，每个工作者完成整个任务。适用于并行、独立的并且没有必要共享状态
   1. 优点。容易理解
   2. 缺点。共享状态可能会很复杂；无状态的工作者(每次都要重读需要的数据)；任务顺序是不确定的
2. 流水线模式。使用非阻塞的IO来设计使用流水线并发模型
   1.  Actors。每个工作者被称为actor,可以直接异步地发送和处理消息
   2.  Channels。工作者之间不直接通信，而是在不同的通道中发布自己的消息
   3.  优点。
       1. 无需共享的状态；
       2. 有状态的工作者；
       3. 较好的硬件整合；
       4. 合理的作业顺序；
   4.  缺点。作业的执行往往分布到多个工作者上。
3. 函数式并行。采用函数调用实现程序，java7中的`ForkAndJoinPool`和java8中的`stream`来并行的实现迭代大型集合。
## volatile
1. 定义：java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。
2. 用于修饰在多线程环境下某一变量值的一致性；
3. 在LinkedTransferQueue中，使用了volatile，追加64字节提高并发编程性能；
## synchronized
- 锁一旦升级为重量级锁，就不会再降级；JVM会尽量维持在低能耗锁
1. 同步的基础
    1. 对于同步方法，锁是当前**实例对象**
    2. 对于静态同步方法，锁是当前对象的**class对象**
    3. 对于同步方法块，锁是Synchonized括号里配置的对象
2. 锁的四种状态
    1. 无状态锁；
    2. 偏向锁状态；
    3. 轻量级锁状态；
    4. 重量级锁状态；
3. 偏向锁
    1. 要解决的问题：大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得。
    2. 锁的获得：测试对象头的Mark Word里是否存在存储着指向当前线程的偏向锁，如不存在，则再测试下Mark Word中偏向锁的标识是否为1，没有设置则使用CAS竞争锁；
    3. 锁的撤销：需要等待全局安全点，它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，必须等到同步代码块执行完；
    4. 关闭偏向锁：在java6和java7中默认启用，通过jvm参数关闭偏向锁：**UseBiasedLocking=false**，那么默认会进入**轻量级锁**状态；
    5. 适用于**只有一个线程**访问同步块；
    6. 偏向锁的耗能比轻量级锁少
4. 轻量级锁
    1. 加锁：线程在执行同步块前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中。
    2. 解锁：使用原子的CAS操作来讲Displaced Mark Word替换回到对象头；
    3. 如果始终得不到锁竞争的线程使用自旋会消耗CPU;
    4. 适用场景：追求响应时间；同步块执行速度非常快；
5. 重量级锁
    1. 线程竞争不使用自旋，不会消耗CPU；
    2. 适用场景：追求吞吐量；同步块执行时间较长
## JAVA线程池的分析和使用
1.  好处：
    1. 降低资源消耗。
    2. 提高响应速度。
    3. 提高线程的可管理性。
2.  使用
             `new ThreadPoolExecutor(corePoolSize,maximumPoolSize,keepAliveTime,milliseconds,runnableTaskQueue,handler)`
    1. corePoolSize(线程池的基本大小)：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，等到需要执行的任务数大于线程池基本大小时不再创建。如果调用了线程池的**prestartAllCoreThreads**方法，线程池会提前创建并启动所有基本线程。
    2. runnableTaskQueue(任务队列)：用于保存等待执行的任务的阻塞队列，队列里处处的是以前提交的任务，需要等待线程空闲时执行。
        * ArrayBlockingQueue：基于数组结构的有界阻塞队列，FIFO（先进先出）
        * LinkedBlockingQueue：基于链表结构的阻塞队列，FIFI（先进先出），Executors.newFixedThreadPool()【设置corePoolSize==maximumPoolSize】使用这个队列
        * SynchronousQueue: 不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，Executors.newCachedThreadPool()使用这个队列
        * PriorityBlockingQueue: 一个具有优先级的无限阻塞队列。
    3. maximumPoolSize(线程池的最大值)   
    4. ThreadFactory：设置创建线程的工厂
    5. RejectedExecutionHandler(饱和策略)：当线程池处于饱和状态时采取的处理新提交的新任务的策略。默认AbortPolicy
        * AbortPolicy: 直接抛出异常；
        * CallerRunsPolicy: 只能调用者所在线程来运行任务；
        * DiscardOldestPolicy: 丢弃队列里最近的一个任务，并执行当前任务；
        * DiscardPolicy: 不处理，丢弃掉
    6. keepAliveTime（线程活动保持时间）：工作线程空闲后，保持存活的时间。
    7. TimeUnit(线程活动保持的单位)
3.  提交任务
    1. execute(): 没有返回值
    2. submit(): 返回Future返回值。*InterruptedException*(处理中断异常)；*ExecutionException*(处理无法执行任务异常)
4.  线程池关闭
    1. 原理：遍历线程池中的工作线程，逐个调用线程的interrupt方法来中断线程
    2. isShutdown(): 调用shutdown和shutdownnow都会变为true；
    3. isTerminaed()：所有任务已关闭后，才表示线程池关闭成功。
5.  线程池流程
    1. 判断基本线程池是否已满；
    2. 判断工作队列是否已满；
    3. 判断整个线程池是否已满；
6.  合理的配置线程池
    1. 分析任务特性
        1. 任务性质：CPU密集型任务，IO密集型任务和混合型任务。
            1. CPU密集型：尽可能小的线程（Ncpu+1），例如压缩和解压缩
            2. IO密集型：尽可能多的线程(2*Ncpu)
            3. `Runtime.getRuntime().availableProcessors()`：当前设备的CPU个数
        2. 任务优先级：高、中、低。使用*PriorityBlockingQueue*队列
        3. 任务的执行时间：长、中、短。使用不同规模的线程池来处理
        4. 任务的依赖性：是否依赖其他系统资源。设置线程数尽可能大。
    2. 建议使用*有界对列*
    3. 动态递增的增加线程池数目
7.  线程池的监控
    1. taskCount: 线程池需要执行的任务数量
    2. completedTaskCount: 线程池在运行过程中已完成的任务数量。
    3. largestTaskCount：曾经创建过的最大线程数量
    4. getPoolSize: 线程池的线程数量
    5. getActiveCount: 获取活动的线程数
    6. 重写线程池的beforeExecute、afterExecute和terminated方法。ø
8.  应急处理：如果在创建线程池的时候，指定的*corePoolSize<maximumPoolsize*是会出现你说的这种情况
## ConcurrentHashMap
1. 在并发情况下不要使用HashMap，导致CPU利用率100%；
2. HashTable容器使用synchronized来保证线程安全，但效率低下，原因是所有访问HashTable线程都必须竞争同一把锁
3. 由*Segment*数组结构和*HashEntry*数组结构组成。Segment是一把ReentrantLock锁
4. get方法，用volatile替换锁，所以读取效率比HashTable高；put方法需要用到锁
## 自旋CAS问题
1. ABA问题：如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值并没有变。*解决思路*：使用版本号。`AtomicStampedReference`来解决ABA问题
2. 循环时间长开销大
3. 只能保证一个共享变量的原子操作。
4. 使用*锁*机制实现原子操作
## ConcurrentLinkedQueue
 1. 基于链接节点的无界线程安全队列，采用先进先出的规则对节点进行排序
 2. 采用"wait-free"算法实现,默认情况下head节点存储的元素为空，tair节点等于head节点
 3. 入队列：将入队节点添加到队列的尾部
## 阻塞队列
1. 定义：一个支持两个附加操作的队列。（在队列为空时，获取元素的线程会等待队列变为非空；当队列满时，存储元素的线程会等待队列可用。）
2. 处理方法
   1. 抛出异常：当阻塞队列满时，往队列里插入元素会抛出异常；当队列为空时，从队列里获取元素时会抛出异常；
   2. 返回特殊值：插入方法会返回是否成功，成功返回TRUE；如果队列里拿出一个元素如果没有则返回null
   3. 一直阻塞：当阻塞队列满时，会阻塞往队列里put的生产者线程；当队列为空，会阻塞消费者线程，直到队列可用
   4. 超时退出：当阻塞队列满时，队列会阻塞生产者线程一段时间，如果超过一定的时间，生产者线程就会退出。
3. 7个阻塞队列
   1. ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列，按照先进先出原则对元素进行排序
   2. LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列，按照先进先出的原则，最大长度为`Integer.MAX_VALUE`
   3. PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。
   4. DelayQueue：一个使用优先级队列实现的无界阻塞队列，必须实现Delayed接口，可以用于缓存系统、定时任务调度
   5. SynchronousQueue：一个不存储元素的阻塞队列，适用于将一个线程使用的数据传递给另一个线程使用
   6. LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。
   7. LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列
4. 实现原理
   1. 使用通知模式实现，当生产者往满的队列里添加元素时会阻塞住生产者，当小费者消费了一个队列中的元素后，会通知生产者当前队列可用。
   2. 利用重入锁(`ReentrantLock`)控制
## Fork/Join框架

1. 定义：用于并行执行任务的框架，是把一个大任务分割成若干个小人物，最终汇总每个小任务结果后得到大任务结果的框架

2. 两个类

   1. ForkJoinTask。(`RecursiveAction`：用于没有返回结果的任务；`RecursiveTask`：用于有返回的任务）
   2. ForkJoinPool

3. 实现原理

   1. ForkJoinPool由`ForkJoinTask`数组[负责存放程序提交给ForkJoinPool的任务]和`ForkJoinWorkerThread`数组[负责执行这些人]组成
   2. ForkJoinTask的fork原理。调用`ForkJoinWorkerThread`的`pushTask`方法，将任务存放在ForkJoinWorkerThread队列里，调用`signalWork()`异步的执行这个任务，然后立即返回结果。
   3. ForkJoinTask的join方法。阻塞当前线程并等待获取结果，四种任务状态【已完成(NORMAL)、被取消(CANCELLED)、信号(SIGNAL)和异常(EXCEPTIONAL)】

   ​